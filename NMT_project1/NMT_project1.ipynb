{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e792093-7db1-4dba-936e-9f7ce27481eb",
   "metadata": {},
   "source": [
    "# NN translator example\n",
    "\n",
    "Reference: https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/\n",
    "\n",
    "Simple German -> English translator using EMbedding, LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a899d554-3544-48a8-965e-ad0306d5965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74283be5-6220-47eb-98e7-04307688f454",
   "metadata": {},
   "source": [
    "## Global options \n",
    "\n",
    "Option_training : whether to train the model or not (T/F)\n",
    "                  if F, load model.h5 file                   \n",
    "Option_evaluation : evaluate the model performance with train/test data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce3652b5-b42c-4c82-a3fd-5a57efd9364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Option_training = False\n",
    "Option_evaluation = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0d281b-6da1-4930-95b9-78dc0ff49e24",
   "metadata": {},
   "source": [
    "## 1. Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1c25fc3-47fd-4336-9511-4dfc46a617f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 여기에서 함수들이 정의됨\n",
    "\n",
    "# load doc into memory\n",
    "# 파일을 string 으로 바꾸어서 return \n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# split a loaded document into sentences\n",
    "# 주어진 doc string 을 먼저 \\n 으로 나누고,\n",
    "# 그 다음은 \\t 으로 나누어서 \n",
    "# pairs = [ [영어문장,독일어문장],[ ..],..] 형식으로 저장 \n",
    "def to_pairs(doc):\n",
    "\tlines = doc.strip().split('\\n')\n",
    "\tpairs = [line.split('\\t') for line in  lines]\n",
    "\treturn pairs\n",
    "\n",
    "# clean a list of lines\n",
    "# 하나의 line = [영어문장, 독일어문장] 형식 \n",
    "# 각 문장에서 \n",
    "# Remove all non-printable characters.\n",
    "# Remove all punctuation characters.\n",
    "# Normalize all Unicode characters to ASCII (e.g. Latin characters).\n",
    "# Normalize the case to lowercase. (이건 독일어에서 고유명사를 구분하는데 문제가 있을 수 있지않을까?) \n",
    "# Remove any remaining tokens that are not alphabetic.\n",
    "def clean_pairs(lines):\n",
    "\tcleaned = list()\n",
    "\t# prepare regex for char filtering\n",
    "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "\t# prepare translation table for removing punctuation\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\tfor pair in lines:\n",
    "\t\tclean_pair = list()\n",
    "\t\tfor line in pair:\n",
    "\t\t\t# normalize unicode characters\n",
    "\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "\t\t\tline = line.decode('UTF-8')\n",
    "\t\t\t# tokenize on white space\n",
    "\t\t\tline = line.split()\n",
    "\t\t\t# convert to lowercase\n",
    "\t\t\tline = [word.lower() for word in line]\n",
    "\t\t\t# remove punctuation from each token\n",
    "\t\t\tline = [word.translate(table) for word in line]\n",
    "\t\t\t# remove non-printable chars form each token\n",
    "\t\t\tline = [re_print.sub('', w) for w in line]\n",
    "\t\t\t# remove tokens with numbers in them\n",
    "\t\t\tline = [word for word in line if word.isalpha()]\n",
    "\t\t\t# store as string\n",
    "\t\t\tclean_pair.append(' '.join(line))\n",
    "\t\tcleaned.append(clean_pair)\n",
    "\treturn array(cleaned)\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "# 저장은 pickle을 이용 \n",
    "def save_clean_data(sentences, filename):\n",
    "\tdump(sentences, open(filename, 'wb'))\n",
    "\tprint('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eddbb57-c0af-4c3a-b2a4-a0d9a7891408",
   "metadata": {},
   "source": [
    "### Create \"english-german.pkl\" file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d4552b9-b19e-4daa-808b-3faf78d5c70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file english-german.pkl exists\n"
     ]
    }
   ],
   "source": [
    "# if pkl file does not exist, create it \n",
    "from pathlib import Path\n",
    "#----target language file---------------\n",
    "filename = 'deu.txt'\n",
    "pkl_file = 'english-german.pkl'\n",
    "#----------------------------------------\n",
    "path = Path(pkl_file)\n",
    "if path.is_file():\n",
    "    print(f'The file {pkl_file} exists')\n",
    "else:\n",
    "    print(f'The file {pkl_file} does not exist')\n",
    "    doc = load_doc(filename)\n",
    "    # split into english-german pairs\n",
    "    pairs = to_pairs(doc)\n",
    "    # clean sentences\n",
    "    clean_pairs = clean_pairs(pairs)\n",
    "    # save clean pairs to file\n",
    "    save_clean_data(clean_pairs, pkl_file)\n",
    "    # spot check\n",
    "    for i in range(100):\n",
    "        print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f6876d-02db-458f-bfbc-80f34eac2470",
   "metadata": {},
   "source": [
    "## 2. Split Text and Simplification\n",
    "\n",
    "* Here only 10,000 examples (among 150,000 phrases) will be used as an example\n",
    "* 9,000 are used for training, 1,000 are for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f5dcbd6-5f5b-47c2-8ef6-3bfc7e41a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "\tdump(sentences, open(filename, 'wb'))\n",
    "\tprint('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7d942c-8ec9-428b-8de4-939ad7957c5d",
   "metadata": {},
   "source": [
    "### save cleaned train/test set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1970a945-5266-41d2-873d-bfc0d12024e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file english-german-both.pkl exists\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "raw_dataset = load_clean_sentences(pkl_file)\n",
    "\n",
    "# reduce dataset size\n",
    "n_sentences = 50000\n",
    "n_train  = 45000\n",
    "\n",
    "path_clean_both = 'english-german-both.pkl'\n",
    "path_clean_train = 'english-german-train.pkl'\n",
    "path_clean_test = 'english-german-test.pkl'\n",
    "path = Path(path_clean_both)\n",
    "\n",
    "def save_clean_data_files(n_sentences,n_train,\n",
    "       path_clean_both,path_clean_train,path_clean_test):\n",
    "    dataset = raw_dataset[:n_sentences, :]\n",
    "    # random shuffle\n",
    "    shuffle(dataset)\n",
    "    # split into train/test\n",
    "    train, test = dataset[:n_train], dataset[n_train:]\n",
    "    # save\n",
    "    save_clean_data(dataset, path_clean_both)\n",
    "    save_clean_data(train, path_clean_train)\n",
    "    save_clean_data(test, path_clean_test)\n",
    "    \n",
    "if path.is_file():\n",
    "    #-check consistency with dataset size \n",
    "    dataset = load_clean_sentences(path_clean_both)\n",
    "    if len(dataset)==n_sentences:\n",
    "        print(f'The file {path_clean_both} exists')\n",
    "    else:\n",
    "        print(f'The file {path_clean_both} mismatch. Re-create it.') \n",
    "        save_clean_data_files(n_sentences,n_train,\n",
    "           path_clean_both,path_clean_train,path_clean_test)\n",
    "else:\n",
    "    print(f'The file {path_clean_both} does not exist. Re-create it')\n",
    "    save_clean_data_files(n_sentences,n_train,\n",
    "           path_clean_both,path_clean_train,path_clean_test)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fe76f5-f2e0-44f6-b175-b6bc1b364cb9",
   "metadata": {},
   "source": [
    "### 3. Train Neural Translation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9ec5ad-3bc7-4f02-b668-c1d80bbad755",
   "metadata": {},
   "source": [
    "The function define_model() below defines the model and takes a number of arguments used to configure the model, such as the size of the input and output vocabularies, the maximum length of input and output phrases, and the number of memory units used to configure the model.\n",
    "\n",
    "The model is trained using the efficient Adam approach to stochastic gradient descent and minimizes the categorical loss function because we have framed the prediction problem as multi-class classification.\n",
    "\n",
    "The model configuration was not optimized for this problem, meaning that there is plenty of opportunity for you to tune it and lift the skill of the translations. I would love to see what you can come up with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cacf1b5f-8fac-4304-814a-7992585fb6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    "\n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "\treturn max(len(line.split()) for line in lines)\n",
    "\n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "\t# integer encode sequences\n",
    "\tX = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad sequences with 0 values\n",
    "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
    "\treturn X\n",
    "\n",
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "\tylist = list()\n",
    "\tfor sequence in sequences:\n",
    "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "\t\tylist.append(encoded)\n",
    "\ty = array(ylist)\n",
    "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "\treturn y\n",
    "\n",
    "# define NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "\tmodel.add(LSTM(n_units))\n",
    "\tmodel.add(RepeatVector(tar_timesteps))\n",
    "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280e51c1-f15a-4a5a-ae32-2ab3066ef804",
   "metadata": {},
   "source": [
    "### Prepare tokenizer/vocabulary and train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a87ad518-2844-4fcf-85a3-1763e4956f08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 6707\n",
      "English Max Length: 8\n",
      "German Vocabulary Size: 11545\n",
      "German Max Length: 17\n",
      " train and test dada prepared. Start model.\n"
     ]
    }
   ],
   "source": [
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')\n",
    "\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))\n",
    "\n",
    "# prepare training data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "# prepare validation data\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)\n",
    "\n",
    "print(' train and test dada prepared. Start model.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640133e4-225d-4b30-b769-cf0708c265f9",
   "metadata": {},
   "source": [
    "### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a34c94d9-0161-4251-9b67-47433d1aa934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 17, 256)           2955520   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 256)               525312    \n",
      "                                                                 \n",
      " repeat_vector (RepeatVector  (None, 8, 256)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 8, 256)            525312    \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 8, 6707)          1723699   \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,729,843\n",
      "Trainable params: 5,729,843\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "plot_model(model, to_file='model.png', show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103aebae-173a-4366-850c-35458c5cc0c2",
   "metadata": {},
   "source": [
    "### fit model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d17e2b3-9f2e-4cb6-9374-3a87e86596bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no new training for the model.\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "filename = 'model.h5'\n",
    "if Option_training :\n",
    "    print('start training the model.')\n",
    "    checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta=0,\n",
    "            patience=0,    verbose=0,    mode='auto',    baseline=None,    restore_best_weights=False)\n",
    "    model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint,earlystop], verbose=2)\n",
    "else:\n",
    "    print('no new training for the model.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351aaa0d-6a2e-4bd0-8d16-6c3037c27112",
   "metadata": {},
   "source": [
    "##  Evaluation NMT\n",
    "\n",
    "If one wants to use existing model, \n",
    "one should skip training part and start from here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29169e12-6210-4980-ac8f-56a804e519cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    "\n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "\treturn max(len(line.split()) for line in lines)\n",
    "\n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "\t# integer encode sequences\n",
    "\tX = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad sequences with 0 values\n",
    "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
    "\treturn X\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == integer:\n",
    "\t\t\treturn word\n",
    "\treturn None\n",
    "\n",
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "\tprediction = model.predict(source, verbose=0)[0]\n",
    "\tintegers = [argmax(vector) for vector in prediction]\n",
    "\ttarget = list()\n",
    "\tfor i in integers:\n",
    "\t\tword = word_for_id(i, tokenizer)\n",
    "\t\tif word is None:\n",
    "\t\t\tbreak\n",
    "\t\ttarget.append(word)\n",
    "\treturn ' '.join(target)\n",
    "\n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "\tactual, predicted = list(), list()\n",
    "\tfor i, source in enumerate(sources):\n",
    "\t\t# translate encoded source text\n",
    "\t\tsource = source.reshape((1, source.shape[0]))\n",
    "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
    "\t\traw_target, raw_src = raw_dataset[i]\n",
    "\t\tif i < 10:\n",
    "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "\t\tactual.append([raw_target.split()])\n",
    "\t\tpredicted.append(translation.split())\n",
    "\t# calculate BLEU score\n",
    "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0294e0db-bbdc-46c4-8a5f-40db0c85871d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no new evaluation of the model.\n"
     ]
    }
   ],
   "source": [
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "# prepare data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "\n",
    "# load model\n",
    "model = load_model('model.h5')\n",
    "if Option_evaluation :\n",
    "    # test on some training sequences\n",
    "    print('evaluate model for train data')\n",
    "    evaluate_model(model, eng_tokenizer, trainX, train)\n",
    "    # test on some test sequences\n",
    "    print('evaluate model for test data')\n",
    "    evaluate_model(model, eng_tokenizer, testX, test)\n",
    "else:\n",
    "    print('no new evaluation of the model.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9119e8-89e4-45d3-9f88-ccdaeb8a1597",
   "metadata": {},
   "source": [
    "## Usage of NMT\n",
    "After training, use the NMT for translation from German-> English \n",
    "\n",
    "For arbitrary German text input,One have to convert German text into NMT input sequence. \n",
    "* (1) clean up text \n",
    "* (2) tokenize the cleaned text \n",
    "* (3) model predicttion for tokenized input\n",
    "* (4) translate output of model to english "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c3cc4df-2e35-4eb2-9579-4b0492893f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "# prepare data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "\n",
    "# load model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "def clean_sentences(sentence_list):\n",
    "    cleaned= list() \n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for line in sentence_list:\n",
    "        # normalize unicode characters\n",
    "        line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "        line = line.decode('UTF-8')\n",
    "        # tokenize on white space\n",
    "        line = line.split()\n",
    "        # convert to lowercase\n",
    "        line = [word.lower() for word in line]\n",
    "        # remove punctuation from each token\n",
    "        line = [word.translate(table) for word in line]\n",
    "        # remove non-printable chars form each token\n",
    "        line = [re_print.sub('', w) for w in line]\n",
    "        # remove tokens with numbers in them\n",
    "        line = [word for word in line if word.isalpha()]\n",
    "        # store as string\n",
    "        cleaned.append(' '.join(line))\n",
    "    return array(cleaned)\n",
    "\n",
    "def translate_german(input_sentences):    \n",
    "    # German input sentence-> cleaning -> tokenized input\n",
    "    # tokenized output -> English \n",
    "    input_sentences_cleaned = clean_sentences(input_sentences)\n",
    "    input_sentences_tokenized = encode_sequences(ger_tokenizer, ger_length, input_sentences_cleaned)\n",
    "    predicted = model.predict(input_sentences_tokenized, verbose=0)\n",
    "    output_sentences = list()\n",
    "    for prediction in predicted:\n",
    "        integers = [argmax(vector) for vector in prediction]\n",
    "        target = list()\n",
    "        for i in integers:\n",
    "            word = word_for_id(i, eng_tokenizer)\n",
    "            if word is None:\n",
    "                break\n",
    "            target.append(word)\n",
    "        output_sentences.append(' '.join(target))\n",
    "    return array(output_sentences)    \n",
    "\n",
    "def translate_german_sentence(input_text):\n",
    "    # same as translate_german \n",
    "    # for one sentence.\n",
    "    input_sentences=[input_text]\n",
    "    return translate_german(input_sentences)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54a7491b-6b95-4b55-9e02-fd2d1b8c39b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i love you'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_german_sentence('Ich liebe dich')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6724128-93b4-4e37-8f29-aabbd5b28e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
